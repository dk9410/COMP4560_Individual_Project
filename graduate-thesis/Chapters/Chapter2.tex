\pagestyle{fancy}
\rhead{\thepage}
\lhead{Literature Review}
\chapter{Literature Review}
\label{ch: chapter 2}

\section{The development of NIME}
\label{subsec: nime}
% what is NIME and it's development
The New Interfaces for Musical Expression (NIME) conference is an international conference for musicians and researchers from all over the world to demonstrate their latest work on musical interface design \citep{Reference15}. It first started as a workshop at the Conference on Human Factors in Computing System (CHI) in 2001. After that, annually conferences have been held around the world. The hoster are research groups who devote themselves to interface design, human-computer interaction and computer music. The latest conference was held at Griffith University in Brisbane, Queensland, Australia in 2016.

In the last sixteen years, NIME has explored different approaches on new musical interface design. The \textit{reacTable} which was designed for live music performance on tabletop led a new trend on tangible music interface \citep{Reference17}. Many researchers shifted their attention to this new media. Toolkit such as reacTiVision was developed to detect movement of performers and allow further development to turn any surface into a musical instrument \citep{Reference18}.

The success of \textit{Smule} initiated a new era of mobile music \citep{Reference8}. After that, thousands of musical applications such as \textit{MoMu, MadPad and Magic Fiddle} which were specifically designed for mobile devices were developed \citep{Reference8.2,Reference8.3,Reference8.4}.


\section{Mobile Music}

With the increasing popularity of mobile device such as smart phone and tablet, a new research field called Mobile Music emerged \citep{Reference4}. According to the definition by \citeauthor{Reference6}, \textit{Mobile Music} wich employing portable technolodge does not only include the scope of playing music, but also involve music composing, synthesizing and sharing\citep{Reference6}.

In the last sixteen years, there is a growing number of researchers start concerning the development of applications in mobile devices. This new trend was first highlighted by \citeauthor{Reference12} after analysing 98 NIME procedding papers related to mobile music during the period from 2002 to 2012\citep{Reference12}.

The expanding capabilities of mobile devices inspired researchers to exploit the new features.The wireless network ability of mobile device is the first area attract researchers' attention. TunA is the first practice of building connection among PDA users though wireless network\citep{Reference7}. By accessing the playlists of nearby users, TunA help users in same network to exchange their music. \citeauthor{Reference5} extended \citeauthor{Reference7}'s work from music sharing towards collaborative musical creation \citep{Reference5}. \citeauthor{Reference5} propsed a system which exploits ad-hoc wireless networks to allow a community of people using their PDA to work on the same piece of music \citep{Reference5}. Some research started from a different approach by investigating the possibility of utilizing the touch screen on the mobile devices. Geiger designed a paradigm for using touch screen on mobile device like iPaq \citep{Reference9, Reference10}.
MoGMI, which stand for Mobile Gesture Music Instrument, is a research project focused on using the accelorometer inside the mobile phone to perform music. Through examing three different axis mapping models, \citeauthor{Reference11} explored how to turn mobile phone into a standard instrument. Smule Ocarina is the most successful mobile musical artifact, which take advantanges of the global popularity of iPhone \citep{Reference8.1}. It leveraged the microphone to take input from breath, and combined with command from the multitouch screen to mimic the physical interaction of ocarina. Besides, Smule Ocarina also utilizing the GPS module to connect users all around the world and create a new social experience \citep{Reference8}.


\section{Musical Interaction Patterns and Sequencer}
% in this section we are going to introduce several major interaction patternn in mobile devices bath on \cite{Reference4}
Musical interaction patterns, also konw as design patterns, are common solutions for developers to design a specific interface,like music sequencer. \citeauthor{Reference4} stated since designer can reuse the proven discipline in their work, design patterns can assisit multidisciplinary design, improve communication between designers and facilaitae knowledage transfer between teams with different background \citep{Reference4}. In \citeauthor{Reference4}'s work, following four most common music interaction patters on mobile devices were given:
1). Natural Interaction. 2). Event Sequencing. 3). Process Control 4). Sound Mixing.
In which, event sequencing was the second most popular interaction patterns. The general decription of event sequencing pattern was illustrated as: editing the sequence of musical event which maybe individual notes, several piece of samples or parameters that can modify the sound of music \citep{Reference4}. In \citeauthor{Reference13}'s paper, sequencer was put into an independent category of musical application on App Store, and it's nature of mapping was briefly discussed.


\section{Evaluation of digital musical instruments}

Digital musical instruments (DMIs) refer to instruments whose sound are generated digitally. It is not uncommon to ask what does evaluation means in the context of digital musical instruments. But as \citeauthor{Reference25} mentioned, evluating the expressivness and creativity of an musical interface were very difficult \citep{Reference25}. \citeauthor{Reference25}'s paper followed by providing a methodology based on discourse analysis. An evaluation framework was given by \citeauthor{Reference22}, in which DMIs were evaluated from four interdependent prosepective: audience, performer, designer and manufacturer \citep{Reference22}. Also, three general design goals were listed at \citeauthor{Reference22}'s paper, which were \textit{Enjoyment, Playability and Robustness}. \citeauthor{Reference23} proposed a process to evaluate DMIs from a performer's view \citep{Reference23}. A case study conducted by \citeauthor{Reference24} was focued on the expressiveness and mapping of DMIs. Recently, by reviewed 89 papers published in NIME from 2012 to 2014, \citeauthor{Reference26} pushed forward the discussion to how to better use the evaluation tools to improve the design of DMIs \citep{Reference26}. \citeauthor{Reference0} proposed a questionnaire to evaluate the experiential qualities of musical instruments in NIME \citep{Reference0}. \citeauthor{Reference0} indicated the following criteria for musicians to perceive musical instruments:
\begin{flushleft}
  \qquad \qquad $\bullet$ \textbf{Experienced freedom and possibilities (EFP)}\\
  \qquad \qquad $\bullet$ \textbf{Perceived control and comfort (PCC)} \\
  \qquad \qquad $\bullet$ \textbf{Perceived stability, sound quality and aesthetics (PSSQA)}\\
\end{flushleft}
\textit{EFP} as the predominent facet, mainly targets at evaluating the musicianship and expressivity of music instruments. For example, questions like\textit{\textquotedblleft{The instrument allows me to express myself.}\textquotedblright} are used to decide whether the instruments can let muscians to express themselves; \textit{PCC} is used to assess the controbility of the music instruments. Questions such as \textit{\textquotedblleft{I can control the sound appropriately.}\textquotedblright} are setted to identify how well the musicians believed they can control the instruments; \textit{PSSQA} is the most unique facet which analyses the quality of the instruments from the material, the sound and the apperience perspectives. For instance, questions like\textit{\textquotedblleft The instrument pleases me sound-wise\textquotedblright} test the sound quality of the instrument. The above three interrelated facets construct the framework of MPX-Q questionnaire.

\clearpage
